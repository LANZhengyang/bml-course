{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the notebook reloads the module each time we modify it\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Uncomment the next line if you want to be able to zoom on plots\n",
    "%matplotlib notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import classification_with_solutions as cl\n",
    "import sklearn as skl\n",
    "#from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = skl.datasets.load_wine(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size, dimension = X.shape\n",
    "print(\"sample size, dimension=\", sample_size, dimension)\n",
    "print(\"class labels are\", np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** perform a PCA, and plot the projection onto the first two PCs. Color your points by class. How easy does the classification task look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick PCA for visualization\n",
    "cl.perform_and_visualize_PCA(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a classification task with 3 classes. The dimension $d$ is pretty high for such a small sample size $n$, it is naturally a task that calls for a Bayesian approach. In the following, we'll fit a simple three-class logistic regression. More precisely, we'll take $b\\in\\mathbb{R}^3$, $\\theta_k\\in\\mathbb{R}^d$ for $k=0,1,2$ and $y\\vert x,\\theta, b$ to be multinomial with parameters that depend smoothly on $x^T\\theta$. More precisely, we consider\n",
    "$$\n",
    "p(y = k \\vert x, b, \\theta) \\propto e^{b_k+\\theta_k^T x}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Give the DAG for the above multinomial regression model.\n",
    "\n",
    "**Question:**  What loss function do you want to take?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we ultimately want to compare the posterior marginals of different components of $b$ and $\\theta$, it is useful to standardize the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessing.scale(X)\n",
    "print(\n",
    "    \"Now every column of X has mean\", np.round(np.mean(X[:,4]), 2), \n",
    "    \"and variance\", np.round(np.var(X[:,7]), 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also want to predict some unknown labels, so let's keep a test set apart. Be careful that comparing average prediction errors with such a small dataset is irrelevant: it is unlikely that they will be good estimators of the generalization error. We will thus just look at confusion matrices and check that our classifiers are not completely off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = skl.model_selection.train_test_split(\n",
    "    X, y, test_size=.2, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple MAP baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try first with `scikit-learn`'s logistic regression. Check out the `multinomial`option, and note how $\\ell_2$ regularization is applied by default, at least in the current version (0.22.2).\n",
    "\n",
    "**Question:** The output of sklearn is thus a MAP estimate, but for what prior? What about the other possible values of the `penalty` option?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skl_intercept, skl_coeffs, skl_predictions = cl.get_sklearn_results(\n",
    "    X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How good are the predicitions?\n",
    "confusion_matrix = skl.metrics.confusion_matrix(y_test, skl_predictions)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the MAP value for the parameters?\n",
    "print(skl_intercept)\n",
    "print(skl_coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we go Bayesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What priors do you want to try? *Hint: make sure the MAP of sklearn is not outside the support of your prior.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Now write your DAG in pymc format, and sample it using NUTS. Put your code in the companion Python file, to make the following line work. Note how there are now two outputs: `trace` and `ppc`. For now just care about `trace` and return whatever for `ppc`, we'll come back to it later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace, ppc = cl.get_logistic_results(X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Check how well you chain has mixed.\n",
    "*Hint: remember the three convergence diagnostics (visual inspection, Gelman-Rubin, Geweke).* \n",
    "\n",
    "**Question:** Plot credible intervals on the parameters. Compare what happens across the three classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "Now let's us predict the labels of the held out \"test\" dataset. \n",
    "\n",
    "**Question:** how do we do this as good Bayesians, now that we have a posterior sample? *Hint: check your course notes for the keyword ``posterior predictive\"*.\n",
    "\n",
    "**Question:** implement the Bayes rule for predictions (actually approximate Bayes rule, since you're going to use the MCMC sample in `trace`). You should complete the function `get_logistic_results` so that it outputs two `pymc3`traces: one of the posterior on the parameters, one of the labels targeting the posterior predictive. Once you have the posterior predictive sample, find the argmax for each test point, and print the confusion matrix."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
