{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian optimization\n",
    "In this practical, we write a class for Gaussian process-based Bayesian optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the notebook reloads the module each time we modify it\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import material for this particular practical. You will have to modify this module.\n",
    "import nonparametric_regression as nr \n",
    "\n",
    "# Make plots look nice\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "# Import generic libraries\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import time\n",
    "\n",
    "# Make plots interactive\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "\n",
    "# Fix a target function\n",
    "target = lambda x: np.abs(x-1/2)*np.cos(10*x)\n",
    "noise = .1\n",
    "noisy_target = lambda x: target(x) + noise*npr.randn(len(x)).reshape(x.shape)\n",
    "\n",
    "# Generate data\n",
    "sample_size = 10\n",
    "dimension = 1\n",
    "X, y = nr.generate_data(noisy_regressed_function = noisy_target, sample_size = sample_size, \n",
    "                        dimension = dimension, seed=seed)\n",
    "\n",
    "#plt.ion() # Keep the plot open, we wull superimpose a few samples afterwards\n",
    "fig, ax = plt.subplots(2, figsize=(10,8))\n",
    "if dimension == 1:\n",
    "    # Plot training data\n",
    "    ax[0].plot(X, y, 'o',label=\"train set\")\n",
    "    \n",
    "    # Plot target function on top\n",
    "    delta = np.max(X)-np.min(X)\n",
    "    plotting_range = (np.min(X)-delta/2, np.max(X)+delta/2)\n",
    "    xPlot = np.linspace(*plotting_range, 100)\n",
    "    ax[0].plot(xPlot, target(xPlot), '--', label=\"target function\", color='r', linewidth=2)\n",
    "    ax[0].legend()\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise:* Leave the above plot open. Now implement a `BayesianOptimization` class in the companion Python file. The object, for starters, should \n",
    "* take $X$ and $y$ as input, \n",
    "* fit the parameters of, say, a Matern kernel,\n",
    "* output the posterior predictive at a given set of test points. \n",
    "\n",
    "Start with using `sklearn`'s GP class, and if you feel like it, remove sklearn once you made everything work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo = nr.BayesianOptimization(X, y)\n",
    "bo.fit() # Fit the GP to the input data\n",
    "\n",
    "if dimension==1:\n",
    "    # Visualize the posterior by showing samples on top of the trainsing data above.\n",
    "    number_of_samples = 10\n",
    "    xPlot = np.linspace(*plotting_range, 200).reshape((200,1))\n",
    "    for seed in range(number_of_samples):\n",
    "        yPlot = bo.sample_y(xPlot, random_state=seed).flatten()\n",
    "        ax[0].plot(xPlot, yPlot, alpha=.3, zorder=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise:* keep the plot open, and superimpose an acquisition criterion in the lower subplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax[1].plot(xPlot, bo.acquisition_criterion(xPlot), label='EI')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise*: Now add methods `find_next_point`and `update` to make the following BO loop work. You should see the added points appear as numbers on the plot above if you left it open. Comment on the behaviour of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo = nr.BayesianOptimization(X, y)\n",
    "bo.fit() # Fit the GP to the input data\n",
    "number_of_iterations = 10\n",
    "\n",
    "for i in range(number_of_iterations):\n",
    "    \n",
    "    # perform one step of BO\n",
    "    xstar = bo.find_next_point(number_of_restarts = 3) # returns the maximum of your acquisition criterion\n",
    "    ystar = noisy_target(xstar)\n",
    "    bo.update(xstar, ystar) # evaluate f at x and add (x, f(x)) to the training sample\n",
    "    bo.fit() # update parameters of the GP\n",
    "\n",
    "    # plot the corresponding sample\n",
    "    ax[0].plot(xstar, ystar, marker='$'+str(i)+'$', color='r')\n",
    "    ax[1].clear()\n",
    "    ax[1].plot(xPlot, bo.acquisition_criterion(xPlot), label='EI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the plot above.\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Bonus exercise:* replace the empirical Bayesian fit of the kernel parameters by a proper MCMC integration. Use `pymc3`. The next point to add to the sample is now optimizing an expected loss. We are still greedy overall instead of solving an (intractable) dynamic programming problem, but this is getting closer to being Bayesian. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
