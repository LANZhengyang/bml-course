Important information for the course will be posted here. Scribes, please use overleaf and the tex template in the notes-by-scribes directory.

## Course notes
* Scribes are taking notes. This is work in progress, and I haven't proofread the notes yet, so use with care. Checkout the [overleaf](https://www.overleaf.com/7327431551dnysmcchtgtt), and feel free to make minor edits.

### Main references, in rough chronologic order of the lectures
* Chapters 7 and 8 of [Parmigiani and Inoue 2009](https://www.webdepot.umontreal.ca/Usagers/perronf/MonDepotPublic/stt2100/Decision_theory.pdf)
* [Wieringen's lecture notes](https://arxiv.org/abs/1509.09169) on ridge regression.
* [Carvalho et al.](https://faculty.chicagobooth.edu/nicholas.polson/research/papers/Horse.pdf)'s paper on the horseshoe prior for sparse regression.
* If you need exercises on Bayesian derivations, check out [The Bayesian core](https://books.google.fr/books/about/Bayesian_Core_A_Practical_Approach_to_Co.html?id=5xwuouehKQoC&redir_esc=y) by Marin and Robert, 2007. The solutions to the exercises are [here](https://arxiv.org/pdf/0910.4696.pdf).
* Savage's axioms are discussed in [Parmigiani and Inoue 2009](https://www.webdepot.umontreal.ca/Usagers/perronf/MonDepotPublic/stt2100/Decision_theory.pdf).
* On MCMC, you can check [(Robert, 2004)](https://www.springer.com/gp/book/9780387212395).
* On variational Bayes, Latent Dirichlet Allocation, and other models for ML, I used [ML: a probabilistic perspective (Murphy, 2012)](https://doc.lagout.org/science/Artificial%20Intelligence/Machine%20learning/Machine%20Learning_%20A%20Probabilistic%20Perspective%20%5BMurphy%202012-08-24%5D.pdf). I understand the subtitle ``A probabilistic perspective" as ``be as Bayesian as you can afford".
